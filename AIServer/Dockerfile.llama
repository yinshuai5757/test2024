# Dockerfile.llama
FROM nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04

ENV MODEL_PATH=/app/text-generation-webui-main/models/MiniCPM-2B-dpo-q4km-gguf.gguf
ENV N_CONTEXT=4096
ENV N_CTX_SIZE=4096
ENV N_GPU_LAYERS=99
ENV DEBIAN_FRONTEND=noninteractive
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda-12.2/targets/x86_64-linux/lib:$LD_LIBRARY_PATH

WORKDIR /app

# 必要なパッケージのインストール
RUN apt-get update && apt-get install -y \
    wget \
    build-essential \
    git \
    curl \
    bzip2 \
    ca-certificates \
    libsndfile1-dev \
    libgl1 \
    python3.11 \
    python3-pip \
    unzip \
    iproute2 \
    vim \
    expect && \
    rm -rf /var/lib/apt/lists/*

# Pythonパッケージのインストール
COPY requirements.txt /app/
RUN pip3 install --no-cache-dir -U pip && \
    pip3 install --no-cache-dir -r requirements.txt

# アプリケーションのダウンロードと展開
RUN git clone --branch b2630 https://github.com/ggerganov/llama.cpp

WORKDIR /app/llama.cpp

# コンパイルして実行
RUN make LLAMA_CUBLAS=1 NVCCFLAGS="-gencode arch=compute_86,code=sm_86"
CMD ./server -c ${N_CONTEXT} --ctx-size ${N_CTX_SIZE} --host 0.0.0.0 -t 8 -m ${MODEL_PATH} --port 7005 -ngl ${N_GPU_LAYERS}
